{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt2-chatbot\n",
    "\n",
    "Following instructions from https://www.kaggle.com/code/pinooxd/gpt2-chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [46 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-310\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\n",
      "      copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
      "      copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
      "      copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
      "      copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
      "      copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
      "      copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
      "      creating build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-310\\tokenizers\\implementations\n",
      "      copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\n",
      "      copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\models\n",
      "      copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\decoders\n",
      "      copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\normalizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\processors\n",
      "      copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-310\\tokenizers\\trainers\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "! pip -q install transformers==2.9.0 gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Imports\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
    "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
    "using a masked language modeling (MLM) loss.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "# try:\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# except ImportError:\n",
    "#     from tensorboardX import SummaryWriter\n",
    "\n",
    "# Configs\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "# Args to allow for easy convertion of python script to notebook\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = 'output'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = 'microsoft/DialoGPT-small'\n",
    "        self.config_name = 'microsoft/DialoGPT-small'\n",
    "        self.tokenizer_name = 'microsoft/DialoGPT-small'\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = False\n",
    "        self.per_gpu_train_batch_size = 4\n",
    "        self.per_gpu_eval_batch_size = 4\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 3\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_steps = 3500\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.no_cuda = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "\n",
    "args = Args()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
